{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "## Contents\n",
    "- Bias-Variance Decomposition\n",
    "    - Mathematical Definition\n",
    "    - Decomposition of Expected Test Error\n",
    "- Ensembles\n",
    "- Bootstrap Aggregating (Bagging)\n",
    "    - Bootstraps\n",
    "    - Result Aggregating\n",
    "- Random Forests\n",
    "    - Variable Importance: OOB\n",
    "- Adaptive Boosting (AdaBoost)\n",
    "    - AdaBoost Algorithm\n",
    "- Gradient Boosting Machine (GBM)\n",
    "- XGBoost - 피피티는 여기까지!\n",
    "- LightGBM\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition\n",
    "\n",
    "Bias-Variance Decomposition 은 모델의 일반화 성능을 높히기 위한 정규화 방법론들과 앙상블 방법론들의 이론적 배경이다. 실제 데이터로 학습되는 모든 모델은 어느정도의 Bias-Variance tradeoff 가 존재하기 때문에, 수리적 배경을 이해하여 모델의 성능을 높히기 위한 적절한 조치를 취해줘야 한다.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"ensemble_images/bvd.png\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "**Error Due to Bias:** 모델의 Expected prediction 과 실제 값의 차이다. 모델의 Expected prediction 이란, 같은 분포에서 나온 많은 데이터셋을 통해 많은 모델을 학습시켰을 때 해당 모델들의 평균 예측치다.\n",
    "\n",
    "**Error Due to Variance:** 같은 분포에서 나온 많은 데이터셋을 통해 모델을 학습시켰을 때, 한 데이터포인트에 대한 전체 평균 예측치와 개별 모델의 예측치의 차이를 의미한다. 즉, 전체 모델들간의 예측치의 차이로 해석할 수 있다.\n",
    "\n",
    "Bias 는 모델들의 전반적인 성능이 좋은지 나쁜지를 의미하고, Variance 는 각 모델들이 서로와 얼마나 다른지를 의미한다. 머신러닝의 대부분의 모델은 둘중 하나가 높다는 특징을 가지고 있기 때문에 Bias-Variance Tradeoff 라고 불린다.\n",
    "\n",
    "특정 모델을 학습시켰을 때 test error 는 필연적으로 등장한다. Bias-Variance Decomposition 은 해당 error 가 Bias 에서 온 error 인지, variance 에서 온 error 인지, noise 에서 온 error 인지 평가해 줄 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Definition\n",
    "\n",
    "> 해당 Tutorial 의 Mathematical Definition 파트는 [Cornell CS4780 SP17](https://www.youtube.com/watch?v=zUJbRO0Wavo) Killian Weinberger 교수님의 강의를 참고하였습니다.\n",
    "\n",
    "$P(X,Y)$ 라는 분포에서 i.i.d 로 데이터셋 $D = \\{(\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_n,y_n)\\}$ 를 추출했다고 가정하자. Regression 모델을 가정했을 때, 모든 벡터 $\\mathbf{x_n}$ 에 대한 정답 라벨 $y_n$ 은 unique 하지 않고, 분포에 따라 주어진다.\n",
    "\n",
    "> 영상에서 이 부분에 대해 갸우뚱하는 학생들을 위해 교수님이 직접 예시를 들어준다. 집에 대한 가격을 나타내는 데이터셋에 대해, 설명변수의 값이 모두 동일한 두개의 집은 무조건적으로 가격이 같지 않다.\n",
    "\n",
    "**Expected Label:**\n",
    "\n",
    "$$\n",
    "\\bar{y}(\\mathbf{x}) = E_{y \\vert \\mathbf{x}} \\left[Y\\right] = \\int\\limits_y y \\, \\Pr(y \\vert \\mathbf{x}) \\partial y\n",
    "$$\n",
    "\n",
    "i.i.d 로 추출한 수많은 데이터셋들에 대해 모델을 학습시킨 뒤, 새로운 샘플 $\\mathbf{x}$ 에 대한 expected label 은 위와 같이 계산할 수 있다. 데이터셋 자체를 $P(X,Y)$ 라는 분포에서 i.i.d 추출했기 때문에 random variable 로 취급할 수 있으므로, continuous r.v. 의 Expectation 을 구하는 것과 동일하게 $y$ 값과 모델의 예측치들에 대해 $y$ 로 미분해주면 된다.\n",
    "\n",
    "**Expected Test Error (given $h_D$):**\n",
    "\n",
    "$$\n",
    "E_{(\\mathbf{x},y) \\sim P} \\left[ \\left(h_D (\\mathbf{x}) - y \\right)^2 \\right] = \\int\\limits_x \\! \\! \\int\\limits_y \\left( h_D(\\mathbf{x}) - y\\right)^2 \\Pr(\\mathbf{x},y) \\partial y \\partial \\mathbf{x}.\n",
    "$$\n",
    "\n",
    "$D$ 라는 학습 데이터를 통해 알고리즘 $\\mathcal{A}$ 를 학습시켰다고 가정하자 ($h_D = \\mathcal{A}(D)$).  그렇다면 해당 모델 $h_D$ 의 Expected Test Error 은 (예시가 regression 이기 때문에 squared loss 사용) 위와 같이 구할 수 있다. $D$ 는 앞서 설명했듯이 random variable 로 취급할 수 있고, $h_D$ 는 $D$ 에 대한 function 이기 때문에 마찬가지로 random variable 이다.\n",
    "\n",
    "**Expected Classifier (given $\\mathcal{A}$):**\n",
    "\n",
    "$$\n",
    "\\bar{h} = E_{D \\sim P^n} \\left[ h_D \\right] = \\int\\limits_D h_D \\Pr(D) \\partial D\n",
    "$$\n",
    "\n",
    "**$h_D$ 와 $\\mathcal{A}$ 를 구분짓는게 핵심이다.** $h_D$ 는 하나의 모델이고 $\\mathcal{A}$ 는 학습될 수 있는 모든 모델이다. 그렇기 때문에 위와 같이 expected classifer 은 분포 $P$ 에서 추출된 $D$ 로 학습된 모든 모델의 expectation 이라고 할 수 있다.\n",
    "\n",
    "**Expected Test Error (given $\\mathcal{A}$):**\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "E_{\\substack{(\\mathbf{x},y) \\sim P\\\\ D \\sim P^n}} \\left[\\left(h_{D}(\\mathbf{x}) - y\\right)^{2}\\right] = \\int_{D} \\int_{\\mathbf{x}} \\int_{y} \\left( h_{D}(\\mathbf{x}) - y\\right)^{2} \\mathrm{P}(\\mathbf{x},y) \\mathrm{P}(D) \\partial \\mathbf{x} \\partial y \\partial D\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "모든 모델에 대한 expectation 을 구했기 때문에 이제 **가능한 모든 모델에 대한 expected error 을 표현 할 수 있다.** 우리의 목적은 $P(X,Y)$ 에서 추출된 $D$ 로 학습된 모델 $\\mathcal{A}$ 의 expected 성능을 구하는 것이기 때문에, 위 식이 이를 나타낸다. 이때 해당 식을 decompose 하면 정확히 어떻게 모델의 에러가 구성되어있는지 볼 수 있다.\n",
    "\n",
    "### Decomposition of Expected Test Error (given $\\mathcal{A}$)\n",
    "\n",
    "$$\n",
    "E_{\\mathbf{x},y,D}\\left[\\left[h_{D}(\\mathbf{x}) - y\\right]^{2}\\right]\n",
    "\\\\\n",
    "\\\\\n",
    "= E_{\\mathbf{x},y,D}\\left[\\left[\\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right) + \\left(\\bar{h}(\\mathbf{x}) - y\\right)\\right]^{2}\\right]\n",
    "\\\\\\\\\n",
    "= E_{\\mathbf{x}, D}\\left[(\\bar{h}_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x}))^{2}\\right]\n",
    "\\\\\n",
    "+2 \\mathrm{\\;} E_{\\mathbf{x}, y, D} \\left[\\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right)\\left(\\bar{h}(\\mathbf{x}) - y\\right)\\right] \n",
    "\\\\\n",
    "+E_{\\mathbf{x}, y} \\left[\\left(\\bar{h}(\\mathbf{x}) - y\\right)^{2}\\right]\n",
    "$$\n",
    "\n",
    "첫 식은 위에서 구한 Expected Test Error 이다. 이때 expectation 안에 모델의 label 의 expectation $\\bar{h}(\\mathbf{x})$ 를 한번씩 빼주고 더해준다. 이를 binomial expansion 을 통해 마지막 식처럼 표현 할 수 있다. 복잡해 보이지만 결국 $(a+b)^2 = a^2+2ab+b^2$ 의 꼴이다. 이때 식의 $2ab$ 부분은 0이 된다.\n",
    "\n",
    "$$\n",
    "E_{\\mathbf{x}, y, D} \\left[ \\left( h_{D}(\\mathbf{x}) - y \\right)^{2} \\right] = \\underbrace{E_{\\mathbf{x}, D} \\left[ \\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x}) \\right)^{2} \\right]}_\\mathrm{Variance} + E_{\\mathbf{x}, y}\\left[ \\left( \\bar{h}(\\mathbf{x}) - y \\right)^{2} \\right]\n",
    "$$\n",
    "\n",
    "Variance 의 정의 자체가 해당 객체가 평균에서 얼마나 분산되어있는지 이기 때문에, 식의 첫번째 부분을 Variance 라고 할 수 있다. 식의 두번째 부분은 처음에 진행 한 것 처럼 binomial expansion 꼴로 표현하면 아래와 같아진다.\n",
    "\n",
    "$$\n",
    "E_{\\mathbf{x}, y} \\left[ \\left(\\bar{h}(\\mathbf{x}) - y \\right)^{2}\\right] = E_{\\mathbf{x}, y} \\left[ \\left(\\bar{h}(\\mathbf{x}) -\\bar y(\\mathbf{x}) )+(\\bar y(\\mathbf{x}) - y \\right)^{2}\\right]  \\\\\\\\\n",
    "=\\underbrace{E_{\\mathbf{x}, y} \\left[\\left(\\bar{y}(\\mathbf{x}) - y\\right)^{2}\\right]}_\\mathrm{Noise} + \\underbrace{E_{\\mathbf{x}} \\left[\\left(\\bar{h}(\\mathbf{x}) - \\bar{y}(\\mathbf{x})\\right)^{2}\\right]}_\\mathrm{Bias^2} \\\\\n",
    "+ 2 \\mathrm{\\;} E_{\\mathbf{x}, y} \\left[ \\left(\\bar{h}(\\mathbf{x}) - \\bar{y}(\\mathbf{x})\\right)\\left(\\bar{y}(\\mathbf{x}) - y\\right)\\right] \n",
    "$$\n",
    "\n",
    "동일하게 $2ab$ 부분은 사라지기 때문에, 최종적으로 expected test error 의 decomposition 은 다음과 같아진다:\n",
    "\n",
    "$$\n",
    "\\underbrace{E_{\\mathbf{x}, y, D} \\left[\\left(h_{D}(\\mathbf{x}) - y\\right)^{2}\\right]}_\\mathrm{Expected\\;Test\\;Error}\n",
    "\\\\\n",
    "= \\underbrace{E_{\\mathbf{x}, D}\\left[\\left(h_{D}(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right)^{2}\\right]}_\\mathrm{Variance} + \\underbrace{E_{\\mathbf{x}, y}\\left[\\left(\\bar{y}(\\mathbf{x}) - y\\right)^{2}\\right]}_\\mathrm{Noise} + \\underbrace{E_{\\mathbf{x}}\\left[\\left(\\bar{h}(\\mathbf{x}) - \\bar{y}(\\mathbf{x})\\right)^{2}\\right]}_\\mathrm{Bias^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**즉, Bias-Variance decomposition 은 한 머신러닝 알고리즘의 expected test error (혹은 성능) 은 3개의 요인으로 나누어 설명할 수 있음을 의미한다. 한개의 모델이 다른 모델들과 얼마나 다른지를 설명하는 Variance, 전체 모델들의 평균 예측치와 실제값들의 평균의 차를 나타내는 Bias (평균 성능이라고 봐도 무방하다), 그리고 설명할 수 없는 data intrinsic 한 noise 의 합으로 하나의 모델의 성능을 나타낼 수 있다.**\n",
    "\n",
    "이상적인 알고리즘은 Bias 와 Variance 가 낮지만, 현실적으로 tradeoff 가 발생하기 때문에 필연적으로 둘중 하나는 크다. 보통 Model Compexity 가 낮은 알고리즘들 (ex. LogReg, LDA) 는 High Bias Low Variance 이며, Model Complexity 가 높은 알고리즘들 (ex. NN, Full DT, SVM) 은 Low Bias High Variance 이다.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"ensemble_images/bvd2.jpg\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "위의 그래프를 보면 Model complexity 가 높을수록 높은 Variance 를 가졌다는 의미가 와닿는다. Training sample 에 대한 에러율은 0에 수렴하지만, 같은 분포에서 추출한 또다른 데이터셋인 testing sample 에 대한 에러는 굉장히 높다. 이는 데이터셋의 작은 변화에도 모델이 민감하게 반응한다는 것을 의미하며, 자주 접하는 과적합이 해당 모델들의 고질적인 문제를 표현한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "이상적인 모델은 Bias 와 Variance 가 낮아야 하지만, 모델의 complexity 에 따라 현실적으론 Bias-Variance Tradeoff 에 의해 둘 중 하나는 필연적으로 높다.\n",
    "\n",
    "이를 해결하기 위해 등장한 방법론들의 모음을 **Ensemble** 이라고 하는데, 이름에서 알 수 있듯이 개별적으로 어느정도의 성능을 내는 여러개의 모델을 합쳐 최종적인 성능을 끌어올리는 방법론들이다. 앙상블 방법론들은 수행하고자 하는 task 로 인해 크개 두가지 갈래로 나눌 수 있다:\n",
    "\n",
    "- Bagging: Decreases Variance\n",
    "\n",
    "- Boosting: Decreases Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Aggregating (Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Data</th>\n",
       "      <th>Bootstrap</th>\n",
       "      <th>1</th>\n",
       "      <th>Bootstrap</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x_1</td>\n",
       "      <td>y_1</td>\n",
       "      <td>x_8</td>\n",
       "      <td>y_8</td>\n",
       "      <td>x_2</td>\n",
       "      <td>y_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x_2</td>\n",
       "      <td>y_2</td>\n",
       "      <td>x_10</td>\n",
       "      <td>y_10</td>\n",
       "      <td>x_1</td>\n",
       "      <td>y_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x_3</td>\n",
       "      <td>y_3</td>\n",
       "      <td>x_8</td>\n",
       "      <td>y_8</td>\n",
       "      <td>x_5</td>\n",
       "      <td>y_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x_4</td>\n",
       "      <td>y_4</td>\n",
       "      <td>x_4</td>\n",
       "      <td>y_4</td>\n",
       "      <td>x_7</td>\n",
       "      <td>y_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x_5</td>\n",
       "      <td>y_5</td>\n",
       "      <td>x_3</td>\n",
       "      <td>y_3</td>\n",
       "      <td>x_2</td>\n",
       "      <td>y_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>x_6</td>\n",
       "      <td>y_6</td>\n",
       "      <td>x_8</td>\n",
       "      <td>y_8</td>\n",
       "      <td>x_9</td>\n",
       "      <td>y_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>x_7</td>\n",
       "      <td>y_7</td>\n",
       "      <td>x_5</td>\n",
       "      <td>y_5</td>\n",
       "      <td>x_2</td>\n",
       "      <td>y_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>x_8</td>\n",
       "      <td>y_8</td>\n",
       "      <td>x_4</td>\n",
       "      <td>y_4</td>\n",
       "      <td>x_7</td>\n",
       "      <td>y_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>x_9</td>\n",
       "      <td>y_9</td>\n",
       "      <td>x_9</td>\n",
       "      <td>y_9</td>\n",
       "      <td>x_4</td>\n",
       "      <td>y_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>x_10</td>\n",
       "      <td>y_10</td>\n",
       "      <td>x_5</td>\n",
       "      <td>y_5</td>\n",
       "      <td>x_3</td>\n",
       "      <td>y_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Original  Data Bootstrap     1 Bootstrap    2\n",
       "0      x_1   y_1       x_8   y_8       x_2  y_2\n",
       "1      x_2   y_2      x_10  y_10       x_1  y_1\n",
       "2      x_3   y_3       x_8   y_8       x_5  y_5\n",
       "3      x_4   y_4       x_4   y_4       x_7  y_7\n",
       "4      x_5   y_5       x_3   y_3       x_2  y_2\n",
       "5      x_6   y_6       x_8   y_8       x_9  y_9\n",
       "6      x_7   y_7       x_5   y_5       x_2  y_2\n",
       "7      x_8   y_8       x_4   y_4       x_7  y_7\n",
       "8      x_9   y_9       x_9   y_9       x_4  y_4\n",
       "9     x_10  y_10       x_5   y_5       x_3  y_3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [f'x_{i}' for i in range(1, 11)]\n",
    "y = [f'y_{i}' for i in range(1, 11)]\n",
    "df = pd.DataFrame({'Original': x, 'Data': y})\n",
    "b1 = df.sample(frac=1, replace=True).reset_index(drop=True)\n",
    "b1.columns=['Bootstrap','1']\n",
    "b2 = df.sample(frac=1, replace=True).reset_index(drop=True)\n",
    "b2.columns=['Bootstrap','2']\n",
    "pd.concat([df, b1, b2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging 은 특정 모델의 분산을 낮춰주는 학습 방법론이다. 위 테이블을 예시로 기존 데이터셋은 10개의 샘플을 가지고 있다. 이때 기존 데이터셋에서 10개의 샘플을 random 으로 복원추출을 하여 $B$ 개의 새로운 데이터셋, 즉 Bootstrap 을 생성한다.\n",
    "\n",
    "해당 Bootstrap 들로 동일한 알고리즘을 학습 한 뒤, 결과를 하나로 합치는 것이 Bagging 의 핵심이다. 이때 알고리즘은 지도학습 알고리즘이면 어떤 것을 사용해도 무방하지만, **Bagging 은 앞서 언급하였듯이 high model complexity 를 가진 알고리즘들에 효과적이다.** Bagging 자체는 알고리즘이 아니라 앙상블의 다양성을 확보하기 위한 방법론 중 하나이기 때문에, 어떤 알고리즘을 사용하냐에 따라 Bagging with Neural Networks, Bagging with SVM 등으로 불리는 명칭이 달라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>92697</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>92037</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>93023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>90034</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>92612</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  \\\n",
       "0        1   25           1      49     91107       4    1.6          1   \n",
       "1        2   45          19      34     90089       3    1.5          1   \n",
       "2        3   39          15      11     94720       1    1.0          1   \n",
       "3        4   35           9     100     94112       1    2.7          2   \n",
       "4        5   35           8      45     91330       4    1.0          2   \n",
       "...    ...  ...         ...     ...       ...     ...    ...        ...   \n",
       "4995  4996   29           3      40     92697       1    1.9          3   \n",
       "4996  4997   30           4      15     92037       4    0.4          1   \n",
       "4997  4998   63          39      24     93023       2    0.3          3   \n",
       "4998  4999   65          40      49     90034       3    0.5          2   \n",
       "4999  5000   28           4      83     92612       3    0.8          1   \n",
       "\n",
       "      Mortgage  Securities Account  CD Account  Online  CreditCard  \n",
       "0            0                   1           0       0           0  \n",
       "1            0                   1           0       0           0  \n",
       "2            0                   0           0       0           0  \n",
       "3            0                   0           0       0           0  \n",
       "4            0                   0           0       0           1  \n",
       "...        ...                 ...         ...     ...         ...  \n",
       "4995         0                   0           0       1           0  \n",
       "4996        85                   0           0       1           0  \n",
       "4997         0                   0           0       0           0  \n",
       "4998         0                   0           0       1           0  \n",
       "4999         0                   0           0       1           1  \n",
       "\n",
       "[5000 rows x 13 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_Personal_Loan_Modelling.csv')\n",
    "X = df.drop('Personal Loan', axis=1)\n",
    "y = df['Personal Loan']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용한 데이터셋은 [Bank Personal Loan Modelling](https://www.kaggle.com/datasets/teertha/personal-loan-modeling) 으로, 총 13개의 설명변수로 해당 고객이 제시된 대출상품을 수락했는지 거절했는지 분류하는 binary classification task 에 사용되는 대표적인 toy dataset 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of a single Logistic regression model: 0.908\n",
      "Test accuracy of a Bagging ensemble of 15 Logistic regression models: 0.913\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "base_model = LogisticRegression()\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "n_estimators = 15\n",
    "ensemble = BaggingClassifier(base_estimator=LogisticRegression(),\n",
    "            n_estimators=n_estimators,\n",
    "            bootstrap=True,\n",
    "            random_state=42)\n",
    "ensemble.fit(X_train, y_train)\n",
    "ensemble.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy of a single Logistic regression model: {base_model.score(X_test, y_test)}')\n",
    "print(f'Test accuracy of a Bagging ensemble of {n_estimators} Logistic regression models: {ensemble.score(X_test, y_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과에서 확인 할 수 있듯이 단일 모델보다 20개의 모델의 앙상블을 사용했을 시 Test accuracy 가 증가한다. 그렇다면 Bagging Ensemble 에서 20개의 모델을 합치는 방법론인 Result Aggregating 을 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Aggregating\n",
    "\n",
    "$B$ 개의 Bootstrap 으로 $B$ 개의 모델을 학습시켰다면, 최종적으로 이 모델들을 하나로 합쳐야 한다. Bagging 에서 이 단계를 Result Aggregating 이라 하며, 다양한 방법론들이 있다.\n",
    "\n",
    "#### Majority Voting\n",
    "\n",
    "각 모델의 결과를 합치는 방법론 중 가장 간단한 방법론인 Majority Voting 은, 과반수의 예측값을 최종 예측값으로 선택하는 것이다.\n",
    "\n",
    "$$\n",
    "\\hat y_{Ensemble}=argmax(\\sum_{j=1}^n\\delta(\\hat y_j=i), \\ i \\in \\{0,1\\})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ensemble Population</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>P(y=1) for a test sample</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model 1</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.140973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model 2</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model 3</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.124695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model 4</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.163355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model 5</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.144366</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model 6</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.196135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model 7</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.118339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model 8</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.122437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model 9</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.151069</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model 10</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.163708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Model 11</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.210730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model 12</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.072664</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Model 13</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.145764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Model 14</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.122003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Model 15</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.147521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ensemble Population  Test Accuracy  P(y=1) for a test sample  \\\n",
       "0              Model 1          0.908                  0.140973   \n",
       "1              Model 2          0.910                  0.149015   \n",
       "2              Model 3          0.907                  0.124695   \n",
       "3              Model 4          0.907                  0.163355   \n",
       "4              Model 5          0.909                  0.144366   \n",
       "5              Model 6          0.908                  0.196135   \n",
       "6              Model 7          0.909                  0.118339   \n",
       "7              Model 8          0.910                  0.122437   \n",
       "8              Model 9          0.903                  0.151069   \n",
       "9             Model 10          0.912                  0.163708   \n",
       "10            Model 11          0.907                  0.210730   \n",
       "11            Model 12          0.945                  0.072664   \n",
       "12            Model 13          0.906                  0.145764   \n",
       "13            Model 14          0.909                  0.122003   \n",
       "14            Model 15          0.908                  0.147521   \n",
       "\n",
       "    Predicted Label  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "5                 0  \n",
       "6                 0  \n",
       "7                 0  \n",
       "8                 0  \n",
       "9                 0  \n",
       "10                0  \n",
       "11                0  \n",
       "12                0  \n",
       "13                0  \n",
       "14                0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample_index = X_test.sample(n=1).index.item()\n",
    "test_sample = X_test.loc[test_sample_index].to_numpy().reshape(1, -1)\n",
    "test_sample_y = y_test.loc[test_sample_index]\n",
    "\n",
    "model_name=[]\n",
    "model_score=[]\n",
    "test_sample_prob=[]\n",
    "predicted_label=[]\n",
    "for i, model in enumerate(ensemble.estimators_):\n",
    "    model_name.append(f'Model {i+1}')\n",
    "    model_score.append(model.score(X_test, y_test))\n",
    "    test_sample_prob.append(model.predict_proba(test_sample)[0][1])\n",
    "    predicted_label.append(model.predict(test_sample)[0])\n",
    "\n",
    "maj_vote = pd.DataFrame({'Ensemble Population':\n",
    "                            model_name,     \n",
    "                        'Test Accuracy':\n",
    "                            model_score,\n",
    "                        'P(y=1) for a test sample':\n",
    "                            test_sample_prob,\n",
    "                        'Predicted Label':\n",
    "                            predicted_label})\n",
    "maj_vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 모델은 새로 들어온 test sample 에 대해 class 1 에 포함될 확률과, 0.5 로 설정된 cutoff 로 sample 에 대한 최종 예측 class 를 보여준다. 이때 Majority Voting 은 단순히 15개의 모델 모두 샘플이 class 0 이라고 예측하였기 때문에, 해당 샘플에 대한 앙상블의 최종 예측값은 0이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Voting\n",
    "각 모델의 결과에 가중치를 부여해서 합칠 수 있는데, 가중치를 각 모델의 Test Accuracy 로 설정한 예시는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\hat y_{Ensemble}=argmax \\left( {\\sum_{j=1}^n(TestAcc_j) \\cdot \\delta(\\hat y_j=i) \\over \\sum_{j=1}^n(TestAcc_j)}, \\ i \\in \\{0,1\\} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta = \\text{Counting function}\n",
    "$$\n",
    "\n",
    "각 class 의 Test Accuracy 의 합을 전체 Test Accuracy 의 합으로 나눠줌으로써 정규화를 진행해준다. 높은 Test Accuracy 는 모델의 신뢰도가 높다는 것을 의미하기 때문에, 이를 가중치로 반영한 방법론이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Vote for Class 0: 1.0\n",
      "Weighted Vote for Class 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "test_sum = sum(maj_vote['Test Accuracy'])\n",
    "test_sum_class_0 = sum(maj_vote.loc[maj_vote['Predicted Label']==0]['Test Accuracy'])\n",
    "test_sum_class_1 = sum(maj_vote.loc[maj_vote['Predicted Label']==1]['Test Accuracy'])\n",
    "weighted_vote_class_0 = test_sum_class_0/test_sum\n",
    "weighted_vote_class_1 = test_sum_class_1/test_sum\n",
    "print(f'Weighted Vote for Class 0: {weighted_vote_class_0}')\n",
    "print(f'Weighted Vote for Class 1: {weighted_vote_class_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Bagging 은 굉장히 직관적인 앙상블 기법이다. 복원추출로 동일한 샘플수를 가진 bootstrap 을 생성하고, 모델들을 학습시킨 뒤 적절히 결합하여 complex 한 모델의 분산을 줄인다. High complexity 를 가진 그 어떤 모델에 bagging 을 적용할 수 있고, base learner (기준이 되는 학습 알고리즘) 에 따라 bagging with SVM 등으로 불린다. 반면 Random Forest 는 **의사결정나무에 bagging 을 적용한 모델로,** 특별히 혼자 다르게 불리는 이유는 OOB 데이터셋에 의한 추가적인 insight 도출 능력과 bagging 에 있어 모든 설명변수를 사용하지 않는다는 특징이 있기 때문이다.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"ensemble_images/rf1.png\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "Random Forest 는 기존 Bagging 을 사용하지만, Bootstrap 을 이용한 학습 단계에서 모든 설명변수를 사용하지 않고 random 한 설명변수의 subset 만으로 학습을 진행한다는 특징을 가지고 있다. 이러한 과정으로 통해 Random Forest 는 Bagging 으로 다양성 확보를 하고, 설명변수를 random 하게 추출함으로써 다양성을 추가적으로 더 확보할 수 있다.\n",
    "\n",
    "> 학부생때 교수님께서 Random Forest 를 설명해주셨을 때, 설명변수의 subset 만으로 학습하는 특징을 '2보 전진을 위한 1보 후퇴' 라고 표현하셨던 것이 기억에 남습니다.\n",
    "\n",
    "예시로 사용할 데이터셋은 [Pumpkin Seeds Dataset](https://www.kaggle.com/datasets/muratkokludataset/pumpkin-seeds-dataset?datasetId=2033872&sortBy=voteCount) 으로, 총 12개의 설명변수로 pumpkin seed 의 종류를 분류하는 binary classification task 에 사용되는 dataset 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Major_Axis_Length</th>\n",
       "      <th>Minor_Axis_Length</th>\n",
       "      <th>Convex_Area</th>\n",
       "      <th>Equiv_Diameter</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>Aspect_Ration</th>\n",
       "      <th>Compactness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56276</td>\n",
       "      <td>888.242</td>\n",
       "      <td>326.1485</td>\n",
       "      <td>220.2388</td>\n",
       "      <td>56831</td>\n",
       "      <td>267.6805</td>\n",
       "      <td>0.7376</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.7453</td>\n",
       "      <td>0.8963</td>\n",
       "      <td>1.4809</td>\n",
       "      <td>0.8207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76631</td>\n",
       "      <td>1068.146</td>\n",
       "      <td>417.1932</td>\n",
       "      <td>234.2289</td>\n",
       "      <td>77280</td>\n",
       "      <td>312.3614</td>\n",
       "      <td>0.8275</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.7151</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>1.7811</td>\n",
       "      <td>0.7487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71623</td>\n",
       "      <td>1082.987</td>\n",
       "      <td>435.8328</td>\n",
       "      <td>211.0457</td>\n",
       "      <td>72663</td>\n",
       "      <td>301.9822</td>\n",
       "      <td>0.8749</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>2.0651</td>\n",
       "      <td>0.6929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66458</td>\n",
       "      <td>992.051</td>\n",
       "      <td>381.5638</td>\n",
       "      <td>222.5322</td>\n",
       "      <td>67118</td>\n",
       "      <td>290.8899</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.7396</td>\n",
       "      <td>0.8486</td>\n",
       "      <td>1.7146</td>\n",
       "      <td>0.7624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66107</td>\n",
       "      <td>998.146</td>\n",
       "      <td>383.8883</td>\n",
       "      <td>220.4545</td>\n",
       "      <td>67117</td>\n",
       "      <td>290.1207</td>\n",
       "      <td>0.8187</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.6752</td>\n",
       "      <td>0.8338</td>\n",
       "      <td>1.7413</td>\n",
       "      <td>0.7557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>79637</td>\n",
       "      <td>1224.710</td>\n",
       "      <td>533.1513</td>\n",
       "      <td>190.4367</td>\n",
       "      <td>80381</td>\n",
       "      <td>318.4289</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.4888</td>\n",
       "      <td>0.6672</td>\n",
       "      <td>2.7996</td>\n",
       "      <td>0.5973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>69647</td>\n",
       "      <td>1084.318</td>\n",
       "      <td>462.9416</td>\n",
       "      <td>191.8210</td>\n",
       "      <td>70216</td>\n",
       "      <td>297.7874</td>\n",
       "      <td>0.9101</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>0.7444</td>\n",
       "      <td>2.4134</td>\n",
       "      <td>0.6433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>87994</td>\n",
       "      <td>1210.314</td>\n",
       "      <td>507.2200</td>\n",
       "      <td>222.1872</td>\n",
       "      <td>88702</td>\n",
       "      <td>334.7199</td>\n",
       "      <td>0.8990</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>0.7643</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>2.2828</td>\n",
       "      <td>0.6599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>80011</td>\n",
       "      <td>1182.947</td>\n",
       "      <td>501.9065</td>\n",
       "      <td>204.7531</td>\n",
       "      <td>80902</td>\n",
       "      <td>319.1758</td>\n",
       "      <td>0.9130</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.7374</td>\n",
       "      <td>0.7185</td>\n",
       "      <td>2.4513</td>\n",
       "      <td>0.6359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>84934</td>\n",
       "      <td>1159.933</td>\n",
       "      <td>462.8951</td>\n",
       "      <td>234.5597</td>\n",
       "      <td>85781</td>\n",
       "      <td>328.8485</td>\n",
       "      <td>0.8621</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.7933</td>\n",
       "      <td>1.9735</td>\n",
       "      <td>0.7104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area  Perimeter  Major_Axis_Length  Minor_Axis_Length  Convex_Area  \\\n",
       "0     56276    888.242           326.1485           220.2388        56831   \n",
       "1     76631   1068.146           417.1932           234.2289        77280   \n",
       "2     71623   1082.987           435.8328           211.0457        72663   \n",
       "3     66458    992.051           381.5638           222.5322        67118   \n",
       "4     66107    998.146           383.8883           220.4545        67117   \n",
       "...     ...        ...                ...                ...          ...   \n",
       "2495  79637   1224.710           533.1513           190.4367        80381   \n",
       "2496  69647   1084.318           462.9416           191.8210        70216   \n",
       "2497  87994   1210.314           507.2200           222.1872        88702   \n",
       "2498  80011   1182.947           501.9065           204.7531        80902   \n",
       "2499  84934   1159.933           462.8951           234.5597        85781   \n",
       "\n",
       "      Equiv_Diameter  Eccentricity  Solidity  Extent  Roundness  \\\n",
       "0           267.6805        0.7376    0.9902  0.7453     0.8963   \n",
       "1           312.3614        0.8275    0.9916  0.7151     0.8440   \n",
       "2           301.9822        0.8749    0.9857  0.7400     0.7674   \n",
       "3           290.8899        0.8123    0.9902  0.7396     0.8486   \n",
       "4           290.1207        0.8187    0.9850  0.6752     0.8338   \n",
       "...              ...           ...       ...     ...        ...   \n",
       "2495        318.4289        0.9340    0.9907  0.4888     0.6672   \n",
       "2496        297.7874        0.9101    0.9919  0.6002     0.7444   \n",
       "2497        334.7199        0.8990    0.9920  0.7643     0.7549   \n",
       "2498        319.1758        0.9130    0.9890  0.7374     0.7185   \n",
       "2499        328.8485        0.8621    0.9901  0.7360     0.7933   \n",
       "\n",
       "      Aspect_Ration  Compactness  \n",
       "0            1.4809       0.8207  \n",
       "1            1.7811       0.7487  \n",
       "2            2.0651       0.6929  \n",
       "3            1.7146       0.7624  \n",
       "4            1.7413       0.7557  \n",
       "...             ...          ...  \n",
       "2495         2.7996       0.5973  \n",
       "2496         2.4134       0.6433  \n",
       "2497         2.2828       0.6599  \n",
       "2498         2.4513       0.6359  \n",
       "2499         1.9735       0.7104  \n",
       "\n",
       "[2500 rows x 12 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Pumpkin_Seeds_Dataset.xlsx')\n",
    "df = df.replace({'Çerçevelik': 0, 'Ürgüp Sivrisi': 1})\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Test accuracy of a single Decision Tree model: 0.848\n",
      "Test accuracy of a Random Forest ensemble of 5 Decision Tree models: 0.856\n",
      "Test accuracy of a Random Forest ensemble of 5 Decision Tree models with Randomized Search: 0.864\n",
      "Parameters of the best Random Forest model: \n",
      " {'n_estimators': 15, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "n_estimators = 5\n",
    "rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=5,random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "n_estimators_random = [i for i in range(1,21)]\n",
    "max_depth = [i for i in range(1, 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "random_grid = {'n_estimators': n_estimators_random,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "rf_base = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf_base, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "best_random.fit(X_train, y_train)\n",
    "\n",
    "print(f'Test accuracy of a single Decision Tree model: {tree.score(X_test, y_test)}')\n",
    "print(f'Test accuracy of a Random Forest ensemble of {n_estimators} Decision Tree models: {rf.score(X_test, y_test)}')\n",
    "print(f'Test accuracy of a Random Forest ensemble of {n_estimators} Decision Tree models with Randomized Search: {best_random.score(X_test, y_test)}')\n",
    "\n",
    "print(f'Parameters of the best Random Forest model: \\n {rf_random.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과에서 확인 할 수 있듯이 단일 모델보다 Random Forest 를 사용했을 시 Test accuracy 가 증가한다. 추가적으로 Random grid search 를 통해 hyperparameter optimization 진행 시, 더욱 좋은 모델이 학습된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance: OOB\n",
    "\n",
    "앙상블 방법론 중 Bagging 은 각 bootstrap 을 만들때 복원추출을 한다. 그렇기에 bootstrap 수가 아무리 많아도, 확률적으로 단 한번도 bootstrap 에 포함되지 않는 샘플들이 있기 마련이다.\n",
    "\n",
    "이를 OOB Data (Out-of-bag) 라고 하며, 앙상블의 개별 모델의 Validation Set 으로 쓰인다. 통상적인 머신러닝 모델을 학습시킬 때 데이터셋을 Train/Validation/Test set 으로 나누어 학습을 진행하지만, OOB의 존재 덕분에 따로 Validation Set 을 빼놓지 않고 보다 더 많은 Training sample 을 쓸 수 있다는 장점이 있다.\n",
    "\n",
    "앙상블 모델중에서 Random Forest 는 실무에서 활용도가 굉장히 높은 모델이다. 바로 OOB Data 로 설명변수의 중요도를 구할 수 있기 때문이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(X.columns[sorted_idx], rf.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Population (앙상블을 구성하는 개별 모델) 에 대해 OOB error ($e_i$) 를 계산한다.\n",
    "\n",
    "**Step 2:** 중요도를 구하고자 하는 설명변수 $x_i$ 의 값들에 대해 random permutation 을 진행 후, permutated OOB error ($p_i$) 를 계산한다.\n",
    "\n",
    "**Step 3:** Population 에 대해 $p_i - e_i$ 의 평균과 분산을 구한 뒤, 변수의 중요도를 산출한다.\n",
    "\n",
    "이때 Step 2 의 random permutation 은 해당 변수에 대한 sample 의 값을 random 하게 섞는 것을 의미한다. 만약 Random Forest 에서 변수 $x_i$ 의 중요도가 높다면, $p_i - e_i$ 의 값은 커야 하며 편차가 적어야 한다.\n",
    "\n",
    "$$\n",
    "d_i^m = p_i^m - e_i^m\n",
    "\\\\\n",
    "\\bar d_i = {1\\over m}\\sum_{i=1}^m d_i^m\n",
    "\\\\\n",
    "s_i^2 = {1 \\over m-1} \\sum_{i=1}^m (d_i^m - \\bar d_i)^2\n",
    "\\\\\n",
    "v_i = {\\bar d_i \\over s_i}\n",
    "$$\n",
    "\n",
    "$d_i^m$ 은 m 번째 tree 에서 설명변수 i 에 대해 permutation 전후 OOB error 의 차이를 나타내고, 이에 따른 평균을 분자로, 분산을 분모로 두어 변수의 중요도 $v_i$ 를 구할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Boosting (AdaBoost)\n",
    "\n",
    "Bagging 은 bootstrap 을 만들어 high model complexity 를 가진 모델을 학습시킨 뒤, 적절히 결합하는 앙상블 방법론이다. **Boosting 은 반대로 하나의 데이터셋에 하나의 모델을 학습시킨 후, 풀어내지 못한 것에 대해 다음단계에서 중점적으로 학습을 반복하는 방법론이다.**\n",
    "\n",
    "Boosting 에선 Strong vs. Weak model 이란 개념이 등장한다. Classifier 기준으로, Weak model 은 random guessing 보다 성능이 아주 약간 좋은 모델을 의미하며, boosting 은 이런 Weak model 을 기준으로 Strong model 을 향해 나아간다. Boosting 은 데이터셋을 통해 성능이 아주 낮은 weak model 을 먼저 학습시킨 뒤, 오분류한 데이터셋 속 샘플에 대해 가중치를 부여하고 새로운 모델의 학습을 반복한다. 이렇기 때문에 bagging 과는 달리 병렬연산이 불가능하다.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"ensemble_images/baggingvboosting.png\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "> Bagging 은 병렬처리가 가능하고, Boosting 은 병렬처리가 불가능하지만 실제로는 Boosting 의 학습이 훨씬 빠른 경우를 종종 볼 수 있다. Bagging 은 애초에 High complexity 를 가진 모델을 병렬 연산하는 것이고, Boosting 은 Weak model 을 직렬연산하는 것이다. 하지만 오히려 Weak model 의 직렬연산이 Bagging 의 high complexity 모델 하나를 학습시키는것보다 빠른 경우가 많다.\n",
    "\n",
    "Adaptive boosting 은 Boosting 방법론 중 weak model 을 stump tree 로 사용하는 앙상블 방법론이다. Stump tree 란 decision boundary split 을 단 1번만 진행한 Decision tree 를 의미하며, (제대로 학습이 이루어졌다고 가정하면) random guessing 보다 성능이 약간 좋다는 특징을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Algorithm\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"ensemble_images/adaboost1.png\" width=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AdaBoost 의 지정 hyperparameter $T$ 는 individual learner 의 개수를 의미한다. AdaBoost 는 직렬연산을 기반한 알고리즘이기 때문에, individual learner 의 개수는 곧 알고리즘의 iteration 을 의미한다. 보통 50~100개를 지정한다.\n",
    "\n",
    "2. AdaBoost 의 Input 은 $N$ 개의 정답 pair 을 가진 학습 데이터셋 $S$ 다. 이때 주목할만한 점은 정답라벨 $y_i$ 다. 일반적인 Classification 을 수행하기 위해 데이터셋의 정답라벨은 $0,1$ 의 값을 갖는데 AdaBoost 의 특성상 $-1, 1$ 을 사용한다. 예측하는데 있어 아무런 차이가 없지만, 이렇게 값을 바꾸어 사용하는 이유는 weight update 에서 확인 할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1300\n",
       " 1    1200\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Pumpkin_Seeds_Dataset.xlsx')\n",
    "df = df.replace({'Çerçevelik': -1, 'Ürgüp Sivrisi': 1})\n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $D_1(i)$ 는 1번째 iteration 에서 sample $i$ 가 선택될 확률분포를 의미한다. **알고리즘 초기엔 일양분포를 사용함으로써 $i$ 가 선택될 확률은 다른 sample 들과 동일하지만, iteration 이 진행되며 이 분포는 바뀐다.**\n",
    "\n",
    "4. for loop 을 이용해서 individual learner 의 개수만큼 알고리즘을 반복한다.\n",
    "\n",
    "   4-1. 확률분포 $D_t$ 를 이용하여 모델 $h_t$ 를 학습시킨다. 여기서 주목할 점은 AdaBoost 의 학습용 데이터셋 생성은 Boosting 과 마찬가지로 복원추출을 사용하지만, 첫번째 iteration 에서만 복원추출하는 방식이 Uniform distribution 이기 때문에 random 하다. 이때 $h_t$ 는 Stump tree 를 의미한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_1 = df.sample(frac=1, replace=True, random_state=42)\n",
    "X_1 = df.drop('Class', axis=1)\n",
    "y_1 = df['Class']\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, random_state=42)\n",
    "tree_1 = DecisionTreeClassifier(max_depth=1)\n",
    "tree_1.fit(X_train_1, y_train_1)\n",
    "plot_tree(tree_1, feature_names=X_1.columns, class_names=['Çerçevelik', 'Ürgüp Sivrisi'], filled=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   4-2. 오분류율인 $\\epsilon_t = P_{D_t}(h_t(x)\\neq y)$ 을 계산한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12549999999999994"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(y_train_1, tree_1.predict(X_train_1))\n",
    "err_1 = 1 - acc\n",
    "err_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   4-3. 오분류율 $\\epsilon_t \\geq 0.5$ 면 알고리즘을 종료하고 다시 처음부터 시작한다. 학습한 모델은 weak model 인 stump tree 이기 때문에, 아무리 성능이 낮을지언정 random guessing 보다 성능이 좋아야 한다. 오분류율이 0.5 보다 크거나 같다는 의미는 stump tree 가 random guessing 보다 못했다는 것을 의미하기 때문에, 처음부터 다시 해보라는 일종의 장치를 걸어둔 것이다.\n",
    "\n",
    "   4-4. $\\alpha_t = {1\\over2}ln({1-\\epsilon_t \\over \\epsilon_t})$ 를 계산하여 가중치 $\\alpha_t$ 를 구한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9706732679434094"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_1 = 0.5 * np.log((1 - err_1) / err_1)\n",
    "alpha_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   4-5. $D_{t+1}(i)= {D_t(i)\\mathbf{exp}(-\\alpha_t y_i h_t (x_i))\\over Z_t}$ 를 통해 다음 iteration 의 샘플들이 선택되는 확률분포를 update 한다. **이 부분을 통해 다음 iteration 에서 샘플들이 복원추출로 선택되는 확률이 달라지는 것이다.** $Z_t$ 는 정규화 factor 이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Major_Axis_Length</th>\n",
       "      <th>Minor_Axis_Length</th>\n",
       "      <th>Convex_Area</th>\n",
       "      <th>Equiv_Diameter</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>Aspect_Ration</th>\n",
       "      <th>Compactness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>106188</td>\n",
       "      <td>1256.137</td>\n",
       "      <td>477.3815</td>\n",
       "      <td>285.4023</td>\n",
       "      <td>107392</td>\n",
       "      <td>367.6993</td>\n",
       "      <td>0.8016</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.7002</td>\n",
       "      <td>0.8457</td>\n",
       "      <td>1.6727</td>\n",
       "      <td>0.7702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>73111</td>\n",
       "      <td>1106.783</td>\n",
       "      <td>466.6600</td>\n",
       "      <td>200.6157</td>\n",
       "      <td>73767</td>\n",
       "      <td>305.1030</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>0.6726</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>2.3261</td>\n",
       "      <td>0.6538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>74188</td>\n",
       "      <td>1139.758</td>\n",
       "      <td>471.4994</td>\n",
       "      <td>203.9162</td>\n",
       "      <td>75631</td>\n",
       "      <td>307.3420</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.6026</td>\n",
       "      <td>0.7177</td>\n",
       "      <td>2.3122</td>\n",
       "      <td>0.6518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>73284</td>\n",
       "      <td>1055.042</td>\n",
       "      <td>410.8323</td>\n",
       "      <td>227.7399</td>\n",
       "      <td>73894</td>\n",
       "      <td>305.4637</td>\n",
       "      <td>0.8323</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>0.8273</td>\n",
       "      <td>1.8040</td>\n",
       "      <td>0.7435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>105158</td>\n",
       "      <td>1271.152</td>\n",
       "      <td>490.9413</td>\n",
       "      <td>273.8355</td>\n",
       "      <td>106080</td>\n",
       "      <td>365.9116</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.6976</td>\n",
       "      <td>0.8178</td>\n",
       "      <td>1.7928</td>\n",
       "      <td>0.7453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>79951</td>\n",
       "      <td>1214.305</td>\n",
       "      <td>520.6570</td>\n",
       "      <td>196.2335</td>\n",
       "      <td>80756</td>\n",
       "      <td>319.0561</td>\n",
       "      <td>0.9263</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.5136</td>\n",
       "      <td>0.6814</td>\n",
       "      <td>2.6533</td>\n",
       "      <td>0.6128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>84429</td>\n",
       "      <td>1171.971</td>\n",
       "      <td>475.0915</td>\n",
       "      <td>229.3297</td>\n",
       "      <td>85639</td>\n",
       "      <td>327.8694</td>\n",
       "      <td>0.8758</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.6709</td>\n",
       "      <td>0.7724</td>\n",
       "      <td>2.0717</td>\n",
       "      <td>0.6901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>68712</td>\n",
       "      <td>1049.409</td>\n",
       "      <td>427.3806</td>\n",
       "      <td>205.9922</td>\n",
       "      <td>69670</td>\n",
       "      <td>295.7817</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.6016</td>\n",
       "      <td>0.7841</td>\n",
       "      <td>2.0747</td>\n",
       "      <td>0.6921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>79991</td>\n",
       "      <td>1155.524</td>\n",
       "      <td>481.8270</td>\n",
       "      <td>212.2153</td>\n",
       "      <td>80847</td>\n",
       "      <td>319.1359</td>\n",
       "      <td>0.8978</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>2.2705</td>\n",
       "      <td>0.6623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>64555</td>\n",
       "      <td>982.531</td>\n",
       "      <td>392.3542</td>\n",
       "      <td>210.6085</td>\n",
       "      <td>65291</td>\n",
       "      <td>286.6949</td>\n",
       "      <td>0.8437</td>\n",
       "      <td>0.9887</td>\n",
       "      <td>0.7475</td>\n",
       "      <td>0.8403</td>\n",
       "      <td>1.8630</td>\n",
       "      <td>0.7307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Area  Perimeter  Major_Axis_Length  Minor_Axis_Length  Convex_Area  \\\n",
       "1235  106188   1256.137           477.3815           285.4023       107392   \n",
       "2474   73111   1106.783           466.6600           200.6157        73767   \n",
       "1228   74188   1139.758           471.4994           203.9162        75631   \n",
       "1118   73284   1055.042           410.8323           227.7399        73894   \n",
       "625   105158   1271.152           490.9413           273.8355       106080   \n",
       "...      ...        ...                ...                ...          ...   \n",
       "2190   79951   1214.305           520.6570           196.2335        80756   \n",
       "808    84429   1171.971           475.0915           229.3297        85639   \n",
       "926    68712   1049.409           427.3806           205.9922        69670   \n",
       "1811   79991   1155.524           481.8270           212.2153        80847   \n",
       "1506   64555    982.531           392.3542           210.6085        65291   \n",
       "\n",
       "      Equiv_Diameter  Eccentricity  Solidity  Extent  Roundness  \\\n",
       "1235        367.6993        0.8016    0.9888  0.7002     0.8457   \n",
       "2474        305.1030        0.9029    0.9911  0.6726     0.7500   \n",
       "1228        307.3420        0.9016    0.9809  0.6026     0.7177   \n",
       "1118        305.4637        0.8323    0.9917  0.7700     0.8273   \n",
       "625         365.9116        0.8300    0.9913  0.6976     0.8178   \n",
       "...              ...           ...       ...     ...        ...   \n",
       "2190        319.0561        0.9263    0.9900  0.5136     0.6814   \n",
       "808         327.8694        0.8758    0.9859  0.6709     0.7724   \n",
       "926         295.7817        0.8762    0.9862  0.6016     0.7841   \n",
       "1811        319.1359        0.8978    0.9894  0.7390     0.7528   \n",
       "1506        286.6949        0.8437    0.9887  0.7475     0.8403   \n",
       "\n",
       "      Aspect_Ration  Compactness  \n",
       "1235         1.6727       0.7702  \n",
       "2474         2.3261       0.6538  \n",
       "1228         2.3122       0.6518  \n",
       "1118         1.8040       0.7435  \n",
       "625          1.7928       0.7453  \n",
       "...             ...          ...  \n",
       "2190         2.6533       0.6128  \n",
       "808          2.0717       0.6901  \n",
       "926          2.0747       0.6921  \n",
       "1811         2.2705       0.6623  \n",
       "1506         1.8630       0.7307  \n",
       "\n",
       "[2000 rows x 12 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_numerator = np.exp(-alpha_1 * y_train_1 * tree_1.predict(X_train_1))\n",
    "d_denominator = sum(d_numerator)\n",
    "d_1 = (d_numerator / d_denominator).to_list()\n",
    "c = random.choices(range(len(X_train_1)), weights=d_1, k=len(X_train_1))\n",
    "X_train_2 = X_train_1.iloc[c]\n",
    "y_train_2 = y_train_1.iloc[c]\n",
    "X_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. $T$ 만큼의 iteration 이 종료되면, 모든 모델 $h_t$ 와 이에 대한 $\\alpha_t$ 값을 더한 뒤, testing sample $(x',y')$ 를 넣어 모델을 검증한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of an AdaBoost Classifier with stump tree as its base classifier: 0.874\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "y.value_counts()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=30, random_state=0)\n",
    "adaboost.fit(X_train, y_train)\n",
    "print(f'Test accuracy of an AdaBoost Classifier with stump tree as its base classifier: {adaboost.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost 의 핵심은 $D_t$ 의 계산에서 볼 수 있다.\n",
    "\n",
    "$$\n",
    "D_{t+1}(i)= {D_t(i)\\mathbf{exp}(-\\alpha_t y_i h_t (x_i))\\over Z_t}\n",
    "$$\n",
    "\n",
    "먼저, $D_t(i)$ 는 $t$ 시점에서 샘플 $i$ 가 선택될 확률을 의미한다.\n",
    "\n",
    "$\\alpha_t$ 의 계산식 ${1\\over2}ln({1-\\epsilon_t \\over \\epsilon_t})$ 를 보면 오분류율에 따라 계산이 되기 때문에, 모델의 정확도가 높을수록 $\\alpha_t$ 가 크다.\n",
    "\n",
    "데이터셋의 정답라벨을 왜 $y_i \\in \\{-1,1\\}$ 으로 설정해야하는지는 $y_i h_t (x_i)$ 의 작동 원리때문이다. $y_i$ 는 샘플 $i$의 정답라벨이고, $h_t(x_i)$ 는 stump tree 의 예측값이다. 만약 샘플 $i$ 에 대해 모델이 올바르게 예측했다면 $y_i h_t (x_i)$ 는 1이 될것이고, 올바르게 예측을 못했다면 $y_i h_t (x_i)$ 는 -1 이 될것이다.\n",
    "\n",
    "그렇기에 $\\mathbf{exp}(-\\alpha_t y_i h_t (x_i))$ 는 **$h_t$ 가 성능이 높고 샘플 $i$ 를 올바르게 예측했다면 $i$ 가 선택될 확률을 줄이는 것이고, $h_t$ 가 정확한 모델임에도 불구하고 $i$ 를 올바르게 예측을 못했다면 $i$ 가 선택될 확률을 다음 단계에서 높히는 것을 의미한다.** 이때 $i$ 가 선택될 확률, 즉 증가/감소 폭은 모델의 성능인 가중치 $\\alpha_t$ 에 의해 결정된다.\n",
    "\n",
    "\n",
    "\n",
    "AdaBoost 는 위와 같은 알고리즘으로 이전단계에 분류를 잘 못한 샘플에 대해 복원추출에서 선택될 확률을 높힘으로써, 모델의 성능을 확보한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine (GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pythonic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9dfe8649efdebf637683ec0ff4ad50ece43b2e2443eff7ae8d3cefb2bef952f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
